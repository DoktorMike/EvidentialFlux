<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>EvidentialFlux.jl · EvidentialFlux</title><script data-outdated-warner src="assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href><img class="docs-light-only" src="assets/logo.png" alt="EvidentialFlux logo"/><img class="docs-dark-only" src="assets/logo-dark.png" alt="EvidentialFlux logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href>EvidentialFlux</a></span></div><form class="docs-search" action="search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li class="is-active"><a class="tocitem" href>EvidentialFlux.jl</a><ul class="internal"><li><a class="tocitem" href="#Deep-Evidential-Regression"><span>Deep Evidential Regression</span></a></li><li><a class="tocitem" href="#Deep-Evidential-Classification"><span>Deep Evidential Classification</span></a></li><li><a class="tocitem" href="#Functions"><span>Functions</span></a></li><li><a class="tocitem" href="#Index"><span>Index</span></a></li><li><a class="tocitem" href="#References"><span>References</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>EvidentialFlux.jl</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>EvidentialFlux.jl</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/DoktorMike/EvidentialFlux.jl/blob/main/docs/src/index.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="EvidentialFlux.jl"><a class="docs-heading-anchor" href="#EvidentialFlux.jl">EvidentialFlux.jl</a><a id="EvidentialFlux.jl-1"></a><a class="docs-heading-anchor-permalink" href="#EvidentialFlux.jl" title="Permalink"></a></h1><p>Evidential Deep Learning is a way to generate predictions and the uncertainty associated with them in one single forward pass. This is in stark contrast to traditional Bayesian neural networks which are typically based on Variational Inference, Markov Chain Monte Carlo, Monte Carlo Dropout or Ensembles.</p><h2 id="Deep-Evidential-Regression"><a class="docs-heading-anchor" href="#Deep-Evidential-Regression">Deep Evidential Regression</a><a id="Deep-Evidential-Regression-1"></a><a class="docs-heading-anchor-permalink" href="#Deep-Evidential-Regression" title="Permalink"></a></h2><p>Deep Evidential Regression<sup class="footnote-reference"><a id="citeref-amini2020" href="#footnote-amini2020">[amini2020]</a></sup> is an attempt to apply the principles of Evidential Deep Learning to regression type problems.</p><p>It works by putting a prior distribution over the likelihood parameters <span>$\mathbf{\theta} = \{\mu, \sigma^2\}$</span> governing a likelihood model where we observe a dataset <span>$\mathcal{D}=\{x_i, y_i\}_{i=1}^N$</span> where <span>$y_i$</span> is assumed to be drawn i.i.d. from a Gaussian distribution.</p><p class="math-container">\[y_i \sim \mathcal{N}(\mu_i, \sigma^2_i)\]</p><p>We can express the posterior parameters <span>$\mathbf{\theta}=\{\mu, \sigma^2\}$</span> as <span>$p(\mathbf{\theta}|\mathcal{D})$</span>. We seek to create an approximation <span>$q(\mu, \sigma^2) = q(\mu)(\sigma^2)$</span> meaning that we assume that the posterior factorizes. This means we can write <span>$\mu\sim\mathcal{N}(\gamma,\sigma^2\nu^{-1})$</span> and <span>$\sigma^2\sim\Gamma^{-1}(\alpha,\beta)$</span>. Thus, we can now form</p><p class="math-container">\[p(\mathbf{\theta}|\mathbf{m})=\mathcal{N}(\gamma,\sigma^2\nu^{-1})\Gamma^{-1}(\alpha,\beta)=\mathcal{N-}\Gamma^{-1}(γ,υ,α,β)\]</p><p>which can be plugged in to the posterior below.</p><p class="math-container">\[p(\mathbf{\theta}|\mathbf{m}, y_i) = \frac{p(y_i|\mathbf{\theta}, \mathbf{m})p(\mathbf{\theta}|\mathbf{m})}{p(y_i|\mathbf{m})}\]</p><p>Now since the likelihood is Gaussian we would like to put a conjugate prior on the parameters of that likelihood and the Normal Inverse Gamma <span>$\mathcal{N-}\Gamma^{-1}(γ, υ, α, β)$</span> fits the bill. I&#39;m being a bit handwavy here but this allows us to express the prediction and the associated uncertainty as below.</p><p class="math-container">\[\underset{Prediction}{\underbrace{\mathbb{E}[\mu]=\gamma}}~~~~
\underset{Aleatoric}{\underbrace{\mathbb{E}[\sigma^2]=\frac{\beta}{\alpha-1}}}~~~~
\underset{Epistemic}{\underbrace{\text{Var}[\mu]=\frac{\beta}{\nu(\alpha-1)}}}\]</p><p>The <code>NIG</code> layer in <code>EvidentialFlux.jl</code> outputs 4 tensors for each target variable, namely <span>$\gamma,\nu,\alpha,\beta$</span>. This means that in one forward pass we can estimate the prediction, the heteroskedastic aleatoric uncertainty as well as the epistemic uncertainty. Boom!</p><h3 id="Theoretical-justifications"><a class="docs-heading-anchor" href="#Theoretical-justifications">Theoretical justifications</a><a id="Theoretical-justifications-1"></a><a class="docs-heading-anchor-permalink" href="#Theoretical-justifications" title="Permalink"></a></h3><p>Although for the problems illustrated by Amini et. al., this approach seems to work well it has been shown in <sup class="footnote-reference"><a id="citeref-nis2022" href="#footnote-nis2022">[nis2022]</a></sup> that there are theoretical shortcomings regarding the expression of the aleatoric and epistemic uncertainty. They propose a correction of the loss, and the uncertainty calculations. In this package I have implemented both.</p><h2 id="Deep-Evidential-Classification"><a class="docs-heading-anchor" href="#Deep-Evidential-Classification">Deep Evidential Classification</a><a id="Deep-Evidential-Classification-1"></a><a class="docs-heading-anchor-permalink" href="#Deep-Evidential-Classification" title="Permalink"></a></h2><p>We follow <sup class="footnote-reference"><a id="citeref-sensoy2018" href="#footnote-sensoy2018">[sensoy2018]</a></sup> in our implementation of Deep Evidential Classification. The neural network layer is implemented to output the <span>$\alpha_k$</span> representing the parameters of a Dirichlet distribution. These parameters has the additional interpretation <span>$\alpha_k = e_k + 1$</span> where <span>$e_k$</span> is the evidence for class <span>$k$</span>. Further, it holds that <span>$e_k &gt; 0$</span> which is the reason for us modeling them with a softplus activation function. </p><p>Ok, so that&#39;s all well and good, but what&#39;s the point? Well, the point is that since we are now constructing a network layer that outputs evidence for each class we can apply Dempster-Shafer Theory (DST) to those outputs. DST is a generalization of the Bayesian framework of thought and works by assigning <code>belief mass</code> to states of interest. We can further concretize this notion by Subjective Logic (SL) which places a Dirichlet distribution over these belief masses. Belief masses are defined as <span>$b_k=e_k/S$</span> where <span>$e_k$</span> is the evidence of state <span>$k$</span> and <span>$S=\sum_i^K(e_i+1)$</span>. Further, SL requires that <span>$K+1$</span> states all sum up to 1. This practically means that <span>$u+\sum_k^K~b_k=1$</span> where <span>$u$</span> represents the uncertainty of the possible K states, or the &quot;I don&#39;t know.&quot; class.</p><p>Now, since <span>$S=\sum_i^K(e_i+1)=S=\sum_i^K(\alpha_i)$</span> SL refers to <span>$S$</span> as the Dirichlet strength which is basically a sum of all the collected evidence in favor of the <span>$K$</span> outcomes. Consequently the uncertainty <span>$u=K/S$</span> becomes 1 in case there is no evidence available. Therefor, <span>$u$</span> is a normalized quantity ranging between 0 and 1.</p><h2 id="Functions"><a class="docs-heading-anchor" href="#Functions">Functions</a><a id="Functions-1"></a><a class="docs-heading-anchor-permalink" href="#Functions" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="EvidentialFlux.DIR" href="#EvidentialFlux.DIR"><code>EvidentialFlux.DIR</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">DIR(in =&gt; out; bias=true, init=Flux.glorot_uniform)
DIR(W::AbstractMatrix, [bias])</code></pre><p>A Linear layer with a softplus activation function in the end to implement the Dirichlet evidential distribution. In this layer the number of output nodes should correspond to the number of classes you wish to model. This layer should be used to model a Multinomial likelihood with a Dirichlet prior. Thus the posterior is also a Dirichlet distribution. Moreover the type II maximum likelihood, i.e., the marginal likelihood is a Dirichlet-Multinomial distribution. Create a fully connected layer which implements the Dirichlet Evidential distribution whose forward pass is simply given by:</p><pre><code class="nohighlight hljs">y = softplus.(W * x .+ bias)</code></pre><p>The input <code>x</code> should be a vector of length <code>in</code>, or batch of vectors represented as an <code>in × N</code> matrix, or any array with <code>size(x,1) == in</code>. The out <code>y</code> will be a vector  of length <code>out</code>, or a batch with <code>size(y) == (out, size(x)[2:end]...)</code> The output will have applied the function <code>softplus(y)</code> to each row/element of <code>y</code>. Keyword <code>bias=false</code> will switch off trainable bias for the layer. The initialisation of the weight matrix is <code>W = init(out, in)</code>, calling the function given to keyword <code>init</code>, with default <a href="@doc Flux.glorot_uniform"><code>glorot_uniform</code></a>. The weight matrix and/or the bias vector (of length <code>out</code>) may also be provided explicitly.</p><p><strong>Arguments:</strong></p><ul><li><code>(in, out)</code>: number of input and output neurons</li><li><code>init</code>: The function to use to initialise the weight matrix.</li><li><code>bias</code>: Whether to include a trainable bias vector.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/DoktorMike/EvidentialFlux.jl/blob/b516ea5b5a7588a960a241ca5f9043d449441fb3/src/dense.jl#L60-L89">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="EvidentialFlux.NIG" href="#EvidentialFlux.NIG"><code>EvidentialFlux.NIG</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">NIG(in =&gt; out, σ=NNlib.softplus; bias=true, init=Flux.glorot_uniform)
NIG(W::AbstractMatrix, [bias, σ])</code></pre><p>Create a fully connected layer which implements the NormalInverseGamma Evidential distribution whose forward pass is simply given by:</p><pre><code class="nohighlight hljs">y = W * x .+ bias</code></pre><p>The input <code>x</code> should be a vector of length <code>in</code>, or batch of vectors represented as an <code>in × N</code> matrix, or any array with <code>size(x,1) == in</code>. The out <code>y</code> will be a vector  of length <code>out*4</code>, or a batch with <code>size(y) == (out*4, size(x)[2:end]...)</code> The output will have applied the function <code>σ(y)</code> to each row/element of <code>y</code> except the first <code>out</code> ones. Keyword <code>bias=false</code> will switch off trainable bias for the layer. The initialisation of the weight matrix is <code>W = init(out*4, in)</code>, calling the function given to keyword <code>init</code>, with default <a href="@doc Flux.glorot_uniform"><code>glorot_uniform</code></a>. The weight matrix and/or the bias vector (of length <code>out</code>) may also be provided explicitly. Remember that in this case the number of rows in the weight matrix <code>W</code> MUST be a multiple of 4. The same holds true for the <code>bias</code> vector.</p><p><strong>Arguments:</strong></p><ul><li><code>(in, out)</code>: number of input and output neurons</li><li><code>σ</code>: The function to use to secure positive only outputs which defaults to the softplus function.</li><li><code>init</code>: The function to use to initialise the weight matrix.</li><li><code>bias</code>: Whether to include a trainable bias vector.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/DoktorMike/EvidentialFlux.jl/blob/b516ea5b5a7588a960a241ca5f9043d449441fb3/src/dense.jl#L1-L27">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="EvidentialFlux.predict" href="#EvidentialFlux.predict"><code>EvidentialFlux.predict</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">predict(m, x)</code></pre><p>Returns the predictions along with the epistemic and aleatoric uncertainty.</p><p><strong>Arguments:</strong></p><ul><li><code>m</code>: the model which has to have the last layer be Normal Inverse Gamma(NIG) layer</li><li><code>x</code>: the input data which has to be given as an array or vector</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/DoktorMike/EvidentialFlux.jl/blob/b516ea5b5a7588a960a241ca5f9043d449441fb3/src/utils.jl#L87-L95">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="EvidentialFlux.uncertainty" href="#EvidentialFlux.uncertainty"><code>EvidentialFlux.uncertainty</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">uncertainty(ν, α, β)</code></pre><p>Calculates the epistemic uncertainty of the predictions from the Normal Inverse Gamma (NIG) model. Given a <span>$\text{N-}\Gamma^{-1}(γ, υ, α, β)$</span> distribution we can calculate the epistemic uncertainty as</p><p><span>$Var[μ] = \frac{β}{ν(α-1)}$</span></p><p><strong>Arguments:</strong></p><ul><li><code>ν</code>: the ν parameter of the NIG distribution which relates to it&#39;s precision and whose shape should be (O, B)</li><li><code>α</code>: the α parameter of the NIG distribution which relates to it&#39;s precision and whose shape should be (O, B)</li><li><code>β</code>: the β parameter of the NIG distribution which relates to it&#39;s uncertainty and whose shape should be (O, B)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/DoktorMike/EvidentialFlux.jl/blob/b516ea5b5a7588a960a241ca5f9043d449441fb3/src/utils.jl#L1-L13">source</a></section><section><div><pre><code class="nohighlight hljs">uncertainty(α, β)</code></pre><p>Calculates the aleatoric uncertainty of the predictions from the Normal Inverse Gamma (NIG) model. Given a <span>$\text{N-}\Gamma^{-1}(γ, υ, α, β)$</span> distribution we can calculate the aleatoric uncertainty as</p><p><span>$\mathbb{E}[σ^2] = \frac{β}{(α-1)}$</span></p><p><strong>Arguments:</strong></p><ul><li><code>α</code>: the α parameter of the NIG distribution which relates to it&#39;s precision and whose shape should be (O, B)</li><li><code>β</code>: the β parameter of the NIG distribution which relates to it&#39;s uncertainty and whose shape should be (O, B)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/DoktorMike/EvidentialFlux.jl/blob/b516ea5b5a7588a960a241ca5f9043d449441fb3/src/utils.jl#L16-L27">source</a></section><section><div><pre><code class="nohighlight hljs">uncertainty(α)</code></pre><p>Calculates the epistemic uncertainty associated with a MultinomialDirichlet model (DIR) layer.</p><ul><li><code>α</code>: the α parameter of the Dirichlet distribution which relates to it&#39;s concentrations and whose shape should be (O, B)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/DoktorMike/EvidentialFlux.jl/blob/b516ea5b5a7588a960a241ca5f9043d449441fb3/src/utils.jl#L30-L36">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="EvidentialFlux.aleatoric" href="#EvidentialFlux.aleatoric"><code>EvidentialFlux.aleatoric</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">aleatoric(ν, α, β)</code></pre><p>This is the aleatoric uncertainty as recommended by Meinert, Nis, Jakob Gawlikowski, and Alexander Lavin. &#39;The Unreasonable Effectiveness of Deep Evidential Regression.&#39; arXiv, May 20, 2022. http://arxiv.org/abs/2205.10060. This is precisely the <span>$σ_{St}$</span> from the Student T distribution.</p><p><strong>Arguments:</strong></p><ul><li><code>ν</code>: the ν parameter of the NIG distribution which relates to it&#39;s precision and whose shape should be (O, B)</li><li><code>α</code>: the α parameter of the NIG distribution which relates to it&#39;s precision and whose shape should be (O, B)</li><li><code>β</code>: the β parameter of the NIG distribution which relates to it&#39;s uncertainty and whose shape should be (O, B)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/DoktorMike/EvidentialFlux.jl/blob/b516ea5b5a7588a960a241ca5f9043d449441fb3/src/utils.jl#L60-L72">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="EvidentialFlux.epistemic" href="#EvidentialFlux.epistemic"><code>EvidentialFlux.epistemic</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">epistemic(ν)</code></pre><p>This is the epistemic uncertainty as recommended by Meinert, Nis, Jakob Gawlikowski, and Alexander Lavin. &#39;The Unreasonable Effectiveness of Deep Evidential Regression.&#39; arXiv, May 20, 2022. http://arxiv.org/abs/2205.10060. </p><p><strong>Arguments:</strong></p><ul><li><code>ν</code>: the ν parameter of the NIG distribution which relates to it&#39;s precision and whose shape should be (O, B)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/DoktorMike/EvidentialFlux.jl/blob/b516ea5b5a7588a960a241ca5f9043d449441fb3/src/utils.jl#L75-L84">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="EvidentialFlux.evidence" href="#EvidentialFlux.evidence"><code>EvidentialFlux.evidence</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">evidence(α)</code></pre><p>Calculates the total evidence of assigning each observation in α to the respective class for a DIR layer.</p><ul><li><code>α</code>: the α parameter of the Dirichlet distribution which relates to it&#39;s concentrations and whose shape should be (O, B)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/DoktorMike/EvidentialFlux.jl/blob/b516ea5b5a7588a960a241ca5f9043d449441fb3/src/utils.jl#L39-L45">source</a></section><section><div><pre><code class="nohighlight hljs">evidence(ν, α)</code></pre><p>Returns the evidence for the data pushed through the NIG layer. In this setting one way of looking at the NIG distribution is as ν virtual observations governing the mean μ of the likelihood and α virtual observations governing the variance <span>$\sigma^2$</span>. The evidence is then a sum of the virtual observations. Amini et. al. goes through this interpretation in their 2020 paper.</p><p><strong>Arguments:</strong></p><ul><li><code>ν</code>: the ν parameter of the NIG distribution which relates to it&#39;s precision and whose shape should be (O, B)</li><li><code>α</code>: the α parameter of the NIG distribution which relates to it&#39;s precision and whose shape should be (O, B)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/DoktorMike/EvidentialFlux.jl/blob/b516ea5b5a7588a960a241ca5f9043d449441fb3/src/utils.jl#L48-L57">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="EvidentialFlux.nigloss" href="#EvidentialFlux.nigloss"><code>EvidentialFlux.nigloss</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">nigloss(y, γ, ν, α, β, λ = 1, ϵ = 0.0001)</code></pre><p>This is the standard loss function for Evidential Inference given a NormalInverseGamma posterior for the parameters of the gaussian likelihood function: μ and σ.</p><p><strong>Arguments:</strong></p><ul><li><code>y</code>: the targets whose shape should be (O, B)</li><li><code>γ</code>: the γ parameter of the NIG distribution which corresponds to it&#39;s mean and whose shape should be (O, B)</li><li><code>ν</code>: the ν parameter of the NIG distribution which relates to it&#39;s precision and whose shape should be (O, B)</li><li><code>α</code>: the α parameter of the NIG distribution which relates to it&#39;s precision and whose shape should be (O, B)</li><li><code>β</code>: the β parameter of the NIG distribution which relates to it&#39;s uncertainty and whose shape should be (O, B)</li><li><code>λ</code>: the weight to put on the regularizer (default: 1)</li><li><code>ϵ</code>: the threshold for the regularizer (default: 0.0001)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/DoktorMike/EvidentialFlux.jl/blob/b516ea5b5a7588a960a241ca5f9043d449441fb3/src/losses.jl#L1-L16">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="EvidentialFlux.nigloss2" href="#EvidentialFlux.nigloss2"><code>EvidentialFlux.nigloss2</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">nigloss2(y, γ, ν, α, β, λ = 1, p = 1)</code></pre><p>This is the corrected loss function for DER as recommended by Meinert, Nis, Jakob Gawlikowski, and Alexander Lavin. “The Unreasonable Effectiveness of Deep Evidential Regression.” arXiv, May 20, 2022. http://arxiv.org/abs/2205.10060. This is the standard loss function for Evidential Inference given a NormalInverseGamma posterior for the parameters of the gaussian likelihood function: μ and σ.</p><p><strong>Arguments:</strong></p><ul><li><code>y</code>: the targets whose shape should be (O, B)</li><li><code>γ</code>: the γ parameter of the NIG distribution which corresponds to it&#39;s mean and whose shape should be (O, B)</li><li><code>ν</code>: the ν parameter of the NIG distribution which relates to it&#39;s precision and whose shape should be (O, B)</li><li><code>α</code>: the α parameter of the NIG distribution which relates to it&#39;s precision and whose shape should be (O, B)</li><li><code>β</code>: the β parameter of the NIG distribution which relates to it&#39;s uncertainty and whose shape should be (O, B)</li><li><code>λ</code>: the weight to put on the regularizer (default: 1)</li><li><code>p</code>: the power which to raise the scaled absolute prediction error (default: 1)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/DoktorMike/EvidentialFlux.jl/blob/b516ea5b5a7588a960a241ca5f9043d449441fb3/src/losses.jl#L34-L52">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="EvidentialFlux.dirloss" href="#EvidentialFlux.dirloss"><code>EvidentialFlux.dirloss</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">dirloss(y, α, t)</code></pre><p>Regularized version of a type II maximum likelihood for the Multinomial(p) distribution where the parameter p, which follows a Dirichlet distribution has been integrated out.</p><p><strong>Arguments:</strong></p><ul><li><code>y</code>: the targets whose shape should be (O, B)</li><li><code>α</code>: the parameters of a Dirichlet distribution representing the belief in each class which shape should be (O, B)</li><li><code>t</code>: counter for the current epoch being evaluated</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/DoktorMike/EvidentialFlux.jl/blob/b516ea5b5a7588a960a241ca5f9043d449441fb3/src/losses.jl#L88-L99">source</a></section></article><h2 id="Index"><a class="docs-heading-anchor" href="#Index">Index</a><a id="Index-1"></a><a class="docs-heading-anchor-permalink" href="#Index" title="Permalink"></a></h2><ul><li><a href="#EvidentialFlux.DIR"><code>EvidentialFlux.DIR</code></a></li><li><a href="#EvidentialFlux.NIG"><code>EvidentialFlux.NIG</code></a></li><li><a href="#EvidentialFlux.aleatoric"><code>EvidentialFlux.aleatoric</code></a></li><li><a href="#EvidentialFlux.dirloss"><code>EvidentialFlux.dirloss</code></a></li><li><a href="#EvidentialFlux.epistemic"><code>EvidentialFlux.epistemic</code></a></li><li><a href="#EvidentialFlux.evidence"><code>EvidentialFlux.evidence</code></a></li><li><a href="#EvidentialFlux.nigloss"><code>EvidentialFlux.nigloss</code></a></li><li><a href="#EvidentialFlux.nigloss2"><code>EvidentialFlux.nigloss2</code></a></li><li><a href="#EvidentialFlux.predict"><code>EvidentialFlux.predict</code></a></li><li><a href="#EvidentialFlux.uncertainty"><code>EvidentialFlux.uncertainty</code></a></li></ul><h2 id="References"><a class="docs-heading-anchor" href="#References">References</a><a id="References-1"></a><a class="docs-heading-anchor-permalink" href="#References" title="Permalink"></a></h2><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-amini2020"><a class="tag is-link" href="#citeref-amini2020">amini2020</a>Amini, Alexander, Wilko Schwarting, Ava Soleimany, and Daniela Rus. “Deep Evidential Regression.” ArXiv:1910.02600 [Cs, Stat], November 24, 2020. http://arxiv.org/abs/1910.02600.</li><li class="footnote" id="footnote-sensoy2018"><a class="tag is-link" href="#citeref-sensoy2018">sensoy2018</a>Sensoy, Murat, Lance Kaplan, and Melih Kandemir. “Evidential Deep Learning to Quantify Classification Uncertainty.” Advances in Neural Information Processing Systems 31 (June 2018): 3179–89.</li><li class="footnote" id="footnote-nis2022"><a class="tag is-link" href="#citeref-nis2022">nis2022</a>Meinert, Nis, Jakob Gawlikowski, and Alexander Lavin. “The Unreasonable Effectiveness of Deep Evidential Regression.” arXiv, May 20, 2022. http://arxiv.org/abs/2205.10060.</li></ul></section></article><nav class="docs-footer"><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.21 on <span class="colophon-date" title="Friday 22 July 2022 22:01">Friday 22 July 2022</span>. Using Julia version 1.6.7.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
